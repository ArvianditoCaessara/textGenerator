{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l9PhAQicEyQd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: filelock in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: requests in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (4.34.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: requests in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: filelock in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macbookpro/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.34.0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /Users/macbookpro/anaconda3/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: sentence-transformers\n"
     ]
    }
   ],
   "source": [
    "!pip3 show transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ItYpaZD9EH7J"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataCollatorForQuestionAnswering' from 'transformers' (/Users/macbookpro/anaconda3/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextDataset, DataCollatorForLanguageModeling, DataCollatorForQuestionAnswering\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2LMHeadModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataCollatorForQuestionAnswering' from 'transformers' (/Users/macbookpro/anaconda3/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import PreTrainedTokenizerFast, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full-text Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nike, Inc.(stylized as NIKE) is an American athletic footwear and apparel corporation headquartered near Beaverton, Oregon, United States. It is the world\\'s largest supplier of athletic shoes and apparel and a major manufacturer of sports equipment, with revenue in excess of US$46 billion in its fiscal year 2022\\nThe company was founded on January 25, 1964, as \"Blue Ribbon Sports\", by Bill Bowerman and Phil Knight, and officially became Nike, Inc. on May 30, 1971. The company takes its name from Nike, the Greek goddess of victory. Nike markets its products under its own brand, as well as Nike Golf, Nike Pro, Nike+, Air Jordan, Nike Blazers, Air Force 1, Nike Dunk, Air Max, Foamposite, Nike Skateboarding, Nike CR7, and subsidiaries including Air Jordan and Converse (brand). Nike also owned Bauer Hockey from 1995 to 2008, and previously owned Cole Haan, Umbro, and Hurley International. In addition to manufacturing sportswear and equipment, the company operates retail stores under the Niketown name. Nike sponsors many high-profile athletes and sports teams around the world, with the highly recognized trademarks of \"Just Do It\" and the Swoosh logo.\\nAs of 2020, it employed 76,700 people worldwide. In 2020, the brand alone was valued in excess of $32 billion, making it the most valuable brand among sports businesses. Previously, in 2017, the Nike brand was valued at $29.6 billion. Nike ranked 89th in the 2018 Fortune 500 list of the largest United States corporations by total revenue.\\nNike, originally known as Blue Ribbon Sports (BRS), was founded by University of Oregon track athlete Phil Knight and his coach, Bill Bowerman, on January 25, 1964. The company initially operated in Eugene, Oregon as a distributor for Japanese shoe maker Onitsuka Tiger, making most sales at track meets out of Knight\\'s automobile.\\nAccording to Otis Davis, a University of Oregon student-athlete coached by Bowerman and Olympic gold medalist at the 1960 Summer Olympics, his coach made the first pair of Nike shoes for him, contradicting a claim that they were made for Phil Knight. According to Davis, \"I told Tom Brokaw that I was the first. I don\\'t care what all the billionaires say. Bill Bowerman made the first pair of shoes for me. People don\\'t believe me. In fact, I didn\\'t like the way they felt on my feet. There was no support and they were too tight. But I saw Bowerman made them from the waffle iron, and they were mine\".\\nIn its first year in business, BRS sold 1,300 pairs of Japanese running shoes grossing $8,000. By 1965, sales had reached $20,000. In 1966, BRS opened its first retail store at 3107 Pico Boulevard in Santa Monica, California. In 1967, due to increasing sales, BRS expanded retail and distribution operations on the East Coast, in Wellesley, Massachusetts.\\nVintage Nike \"waffle racer\" sneaker. In 1971, Bowerman used his wife\\'s waffle iron to experiment on rubber to create a new sole for track shoes that would grip but be lightweight and increase the runner\\'s speed. Oregon\\'s Hayward Field was transitioning to an artificial surface, and Bowerman wanted a sole which could grip to grass or bark dust without the use of spikes. Bowerman was talking to his wife about this puzzle over breakfast, when the waffle iron idea came into play.\\nBowerman\\'s design led to the introduction of the \"Moon Shoe\" in 1972, so named because the waffle tread was said to resemble the footprints left by astronauts on the moon. Further refinement resulted in the \"Waffle Trainer\" in 1974, which helped fuel the explosive growth of Blue Ribbon Sports/Nike.\\nTension between BRS and Onitsuka Tiger increased in 1971 as the latter attempted a takeover of BRS by extending an ultimatum proposal that would give the Japanese company 51 percent of BRS. In 1972, the relationship between BRS and Onitsuka Tiger came to an end. BRS prepared to launch its own line of footwear. The previous year, it was already able to place from two Japanese shoe manufacturers the company’s first independent order for 20,000, which included 6,000 that had the Nike logo. Runner Jeff Johnson was brought in to help market the new brand and was credited for coining the name “Nike”. It would bear the Swoosh newly designed by Carolyn Davidson. The Swoosh was first used by Nike on June 18, 1971, and was registered with the U.S. Patent and Trademark Office on January 22, 1974.\\nIn 1976, the company hired John Brown and Partners, based in Seattle, as its first advertising agency. The following year, the agency created the first \"brand ad\" for Nike, called \"There is no finish line\", in which no Nike product was shown. By 1980 Nike had attained a 50% market share in the U.S. athletic shoe market, and the company went public in December of that year.\\nWieden+Kennedy, Nike\\'s primary ad agency, has worked with Nike to create many print and television advertisements, and Wieden+Kennedy remains Nike\\'s primary ad agency. It was agency co-founder Dan Wieden who coined the now-famous slogan \"Just Do It\" for a 1988 Nike ad campaign, which was chosen by Advertising Age as one of the top five ad slogans of the 20th century and enshrined in the Smithsonian Institution. Walt Stack was featured in Nike\\'s first \"Just Do It\" advertisement, which debuted on July 1, 1988. Wieden credits the inspiration for the slogan to \"Let\\'s do it\", the last words spoken by Gary Gilmore before he was executed.\\nThroughout the 1980s, Nike expanded its product line to encompass many sports and regions throughout the world. In 1990, Nike moved into its eight-building World Headquarters campus in Beaverton, Oregon. The first Nike retail store, dubbed Niketown, opened in downtown Portland in November of that year.\\nPhil Knight announced in mid-2015 that he would step down as chairman of Nike in 2016. He officially stepped down from all duties with the company on June 30, 2016.\\nIn a company public announcement on March 15, 2018, Nike CEO Mark Parker said Trevor Edwards, a top Nike executive who was seen as a potential successor to the chief executive, was relinquishing his position as Nike\\'s brand president and would retire in August.       \\nIn October 2019, John Donahoe was announced as the next CEO, and succeeded Parker on January 13, 2020. In November 2019, the company stopped selling directly through Amazon, focusing more on direct relationships with customers.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read Full Text Data\n",
    "df = pd.read_csv('data/nike_data_fulltext.csv')\n",
    "data = df[\"Text\"].str.cat(sep='\\n')\n",
    "text_data = re.sub(r'\\n+', '\\n', data).strip()  # Remove excess newline characters\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the full text training data\n",
    "with open(\"data/train_fulltext.txt\", \"w\") as f:\n",
    "    f.write(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_file_path,model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "    data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "                      output_dir=output_dir,\n",
    "                      overwrite_output_dir=overwrite_output_dir,\n",
    "                      per_device_train_batch_size=per_device_train_batch_size,\n",
    "                      num_train_epochs=num_train_epochs,\n",
    "                    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "                  model=model,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=train_dataset,\n",
    "              )\n",
    "      \n",
    "    trainer.train()\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "train_file_path = \"data/train_fulltext.txt\"\n",
    "model_name = 'gpt2'\n",
    "output_dir = 'model/model_fulltext'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 50.0\n",
    "save_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /Users/macbookpro/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/vocab.json\n",
      "loading file merges.txt from cache at /Users/macbookpro/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/macbookpro/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Loading features from cached file data/cached_lm_GPT2Tokenizer_128_train_fulltext.txt [took 0.000 s]\n",
      "tokenizer config file saved in model/model_fulltext/tokenizer_config.json\n",
      "Special tokens file saved in model/model_fulltext/special_tokens_map.json\n",
      "loading configuration file config.json from cache at /Users/macbookpro/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/macbookpro/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Configuration saved in model/model_fulltext/config.json\n",
      "Model weights saved in model/model_fulltext/pytorch_model.bin\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 11\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 124439808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 12:45, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to model/model_fulltext\n",
      "Configuration saved in model/model_fulltext/config.json\n",
      "Model weights saved in model/model_fulltext/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "def generate_text(model_path, sequence, max_length):\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model/model_fulltext/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model/model_fulltext/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at model/model_fulltext.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q] When was the company founded? [A] August 19, 1997 [b] When was the company founded? [A] September 13, 2003 [b] When was the company founded? [A] October 1, 2001 [b\n"
     ]
    }
   ],
   "source": [
    "#Config\n",
    "model1_path = 'model/model_fulltext'\n",
    "sequence1 = \"[Q] When was the company founded?\"\n",
    "max_len = 50\n",
    "generate_text(model1_path, sequence1, max_len) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune with Q and A format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the data\n",
    "df = pd.read_csv('data/nike_data_qanda.csv')\n",
    "data = df[\"Text\"].str.cat(sep='\\n')\n",
    "text_data = re.sub(r'\\n+', '\\n', data).strip()  # Remove excess newline characters\n",
    "\n",
    "with open(\"data/finetune_qanda.txt\", \"w\") as f:\n",
    "    f.write(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(finetune_file_path,model_name,\n",
    "              model_path,\n",
    "              output_dir,\n",
    "              overwrite_output_dir,\n",
    "              per_device_train_batch_size,\n",
    "              num_train_epochs,\n",
    "              save_steps):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    train_dataset = load_dataset(finetune_file_path, tokenizer)\n",
    "    data_collator = load_data_finetuned_collator(tokenizer)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "    model = load_model(model_path)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "                      output_dir=output_dir,\n",
    "                      overwrite_output_dir=overwrite_output_dir,\n",
    "                      per_device_train_batch_size=per_device_train_batch_size,\n",
    "                      num_train_epochs=num_train_epochs,\n",
    "                    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "                  model=model,\n",
    "                  args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=train_dataset,\n",
    "              )\n",
    "      \n",
    "    trainer.train()\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_finetuned_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "finetune_file_path = \"data/finetune_qanda.txt\"\n",
    "model_path = 'model/model_fulltext'\n",
    "output_dir = 'model/model_finetuned_qanda'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 50.0\n",
    "save_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /Users/macbookpro/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/vocab.json\n",
      "loading file merges.txt from cache at /Users/macbookpro/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/macbookpro/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Loading features from cached file data/cached_lm_GPT2Tokenizer_128_finetune_qanda.txt [took 0.000 s]\n",
      "tokenizer config file saved in model/model_finetuned_qanda/tokenizer_config.json\n",
      "Special tokens file saved in model/model_finetuned_qanda/special_tokens_map.json\n",
      "loading configuration file model/model_fulltext/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model/model_fulltext/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at model/model_fulltext.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Configuration saved in model/model_finetuned_qanda/config.json\n",
      "Model weights saved in model/model_finetuned_qanda/pytorch_model.bin\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/macbookpro/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 124439808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 15:48, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to model/model_finetuned_qanda\n",
      "Configuration saved in model/model_finetuned_qanda/config.json\n",
      "Model weights saved in model/model_finetuned_qanda/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Fine-Tune\n",
    "fine_tune(\n",
    "    finetune_file_path=finetune_file_path,\n",
    "    model_name=model_name,\n",
    "    model_path=model_path, \n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSAOQPk8vOTE",
    "outputId": "69bedc13-2ac6-4ccc-c24a-dde11ef2015d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/06/g26v_t9960g7byk34gcmfn7h0000gn/T/ipykernel_9659/393270789.py\", line 4, in <module>\n",
      "    generate_text(model2_path, sequence2, max_len)\n",
      "  File \"/var/folders/06/g26v_t9960g7byk34gcmfn7h0000gn/T/ipykernel_9659/1148284488.py\", line 13, in generate_text\n",
      "    model = load_model(model_path)\n",
      "  File \"/var/folders/06/g26v_t9960g7byk34gcmfn7h0000gn/T/ipykernel_9659/1148284488.py\", line 3, in load_model\n",
      "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1966, in from_pretrained\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 532, in from_pretrained\n",
      "    'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 559, in get_config_dict\n",
      "    by the `return_unused_kwargs` keyword parameter.\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 614, in _get_config_dict\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 380, in cached_file\n",
      "OSError: model/model_finetuned_qanda does not appear to have a file named config.json. Checkout 'https://huggingface.co/model/model_finetuned_qanda/None' for available files.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/macbookpro/anaconda3/lib/python3.10/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "model2_path = 'model/model_finetuned_qanda'\n",
    "sequence2 = \"[Q] Who is Nike CEO?\"\n",
    "max_len = 30\n",
    "generate_text(model2_path, sequence2, max_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nike, Inc.(stylized as NIKE) is an American athletic footwear and apparel corporation headquartered near Beaverton, Oregon, United States. It is the world\\'s largest supplier of athletic shoes and apparel and a major manufacturer of sports equipment, with revenue in excess of US$46 billion in its fiscal year 2022\\nThe company was founded on January 25, 1964, as \"Blue Ribbon Sports\", by Bill Bowerman and Phil Knight, and officially became Nike, Inc. on May 30, 1971. The company takes its name from Nike, the Greek goddess of victory. Nike markets its products under its own brand, as well as Nike Golf, Nike Pro, Nike+, Air Jordan, Nike Blazers, Air Force 1, Nike Dunk, Air Max, Foamposite, Nike Skateboarding, Nike CR7, and subsidiaries including Air Jordan and Converse (brand). Nike also owned Bauer Hockey from 1995 to 2008, and previously owned Cole Haan, Umbro, and Hurley International. In addition to manufacturing sportswear and equipment, the company operates retail stores under the Niketown name. Nike sponsors many high-profile athletes and sports teams around the world, with the highly recognized trademarks of \"Just Do It\" and the Swoosh logo.\\nAs of 2020, it employed 76,700 people worldwide. In 2020, the brand alone was valued in excess of $32 billion, making it the most valuable brand among sports businesses. Previously, in 2017, the Nike brand was valued at $29.6 billion. Nike ranked 89th in the 2018 Fortune 500 list of the largest United States corporations by total revenue.\\nNike, originally known as Blue Ribbon Sports (BRS), was founded by University of Oregon track athlete Phil Knight and his coach, Bill Bowerman, on January 25, 1964. The company initially operated in Eugene, Oregon as a distributor for Japanese shoe maker Onitsuka Tiger, making most sales at track meets out of Knight\\'s automobile.\\nAccording to Otis Davis, a University of Oregon student-athlete coached by Bowerman and Olympic gold medalist at the 1960 Summer Olympics, his coach made the first pair of Nike shoes for him, contradicting a claim that they were made for Phil Knight. According to Davis, \"I told Tom Brokaw that I was the first. I don\\'t care what all the billionaires say. Bill Bowerman made the first pair of shoes for me. People don\\'t believe me. In fact, I didn\\'t like the way they felt on my feet. There was no support and they were too tight. But I saw Bowerman made them from the waffle iron, and they were mine\".\\nIn its first year in business, BRS sold 1,300 pairs of Japanese running shoes grossing $8,000. By 1965, sales had reached $20,000. In 1966, BRS opened its first retail store at 3107 Pico Boulevard in Santa Monica, California. In 1967, due to increasing sales, BRS expanded retail and distribution operations on the East Coast, in Wellesley, Massachusetts.\\nVintage Nike \"waffle racer\" sneaker. In 1971, Bowerman used his wife\\'s waffle iron to experiment on rubber to create a new sole for track shoes that would grip but be lightweight and increase the runner\\'s speed. Oregon\\'s Hayward Field was transitioning to an artificial surface, and Bowerman wanted a sole which could grip to grass or bark dust without the use of spikes. Bowerman was talking to his wife about this puzzle over breakfast, when the waffle iron idea came into play.\\nBowerman\\'s design led to the introduction of the \"Moon Shoe\" in 1972, so named because the waffle tread was said to resemble the footprints left by astronauts on the moon. Further refinement resulted in the \"Waffle Trainer\" in 1974, which helped fuel the explosive growth of Blue Ribbon Sports/Nike.\\nTension between BRS and Onitsuka Tiger increased in 1971 as the latter attempted a takeover of BRS by extending an ultimatum proposal that would give the Japanese company 51 percent of BRS. In 1972, the relationship between BRS and Onitsuka Tiger came to an end. BRS prepared to launch its own line of footwear. The previous year, it was already able to place from two Japanese shoe manufacturers the company’s first independent order for 20,000, which included 6,000 that had the Nike logo. Runner Jeff Johnson was brought in to help market the new brand and was credited for coining the name “Nike”. It would bear the Swoosh newly designed by Carolyn Davidson. The Swoosh was first used by Nike on June 18, 1971, and was registered with the U.S. Patent and Trademark Office on January 22, 1974.\\nIn 1976, the company hired John Brown and Partners, based in Seattle, as its first advertising agency. The following year, the agency created the first \"brand ad\" for Nike, called \"There is no finish line\", in which no Nike product was shown. By 1980 Nike had attained a 50% market share in the U.S. athletic shoe market, and the company went public in December of that year.\\nWieden+Kennedy, Nike\\'s primary ad agency, has worked with Nike to create many print and television advertisements, and Wieden+Kennedy remains Nike\\'s primary ad agency. It was agency co-founder Dan Wieden who coined the now-famous slogan \"Just Do It\" for a 1988 Nike ad campaign, which was chosen by Advertising Age as one of the top five ad slogans of the 20th century and enshrined in the Smithsonian Institution. Walt Stack was featured in Nike\\'s first \"Just Do It\" advertisement, which debuted on July 1, 1988. Wieden credits the inspiration for the slogan to \"Let\\'s do it\", the last words spoken by Gary Gilmore before he was executed.\\nThroughout the 1980s, Nike expanded its product line to encompass many sports and regions throughout the world. In 1990, Nike moved into its eight-building World Headquarters campus in Beaverton, Oregon. The first Nike retail store, dubbed Niketown, opened in downtown Portland in November of that year.\\nPhil Knight announced in mid-2015 that he would step down as chairman of Nike in 2016. He officially stepped down from all duties with the company on June 30, 2016.\\nIn a company public announcement on March 15, 2018, Nike CEO Mark Parker said Trevor Edwards, a top Nike executive who was seen as a potential successor to the chief executive, was relinquishing his position as Nike\\'s brand president and would retire in August.       \\nIn October 2019, John Donahoe was announced as the next CEO, and succeeded Parker on January 13, 2020. In November 2019, the company stopped selling directly through Amazon, focusing more on direct relationships with customers.\\nAcquisition. \\nNike has acquired and sold several apparel and footwear companies over the course of its history. Its first acquisition was the upscale footwear company Cole Haan in 1988, followed by the purchase of Bauer Hockey in 1994. In 2002, Nike bought surf apparel company Hurley International from founder Bob Hurley. In 2003, Nike paid US$309 million to acquire sneaker company Converse.[47] The company acquired Starter in 2004[48] and soccer uniform maker Umbro in 2007.\\nIn order to refocus its business lines, Nike began divesting itself of some of its subsidiaries in the 2000s. It sold Starter in 2007 and Bauer Hockey in 2008. The company sold Umbro in 2012 and Cole Haan in 2013. As of 2020, Nike owns only one subsidiary: Converse Inc.\\nNike acquired Zodiac, a consumer data analytics company, in March 2018. In August 2019, the company acquired Celect, a Boston-based predictive analytics company. In December 2021, Nike purchased RTFKT Studios, a virtual shoe company that makes NFTs.\\nIn February 2021, Nike acquired Datalogue, a New York based company focused on digital sales and machine learning technology\\nFinance. Nike was made a member of the Dow Jones Industrial Average in 2013, when it replaced Alcoa.\\nOn December 19, 2013, Nike\\'s quarterly profit rose due to a 13 percent increase in global orders for merchandise since April of that year.[58] Future orders of shoes or clothes for delivery between December and April, rose to $10.4 billion. Nike shares (NKE) rose 0.6 percent to $78.75 in extended trading.\\nIn November 2015, Nike announced it would initiate a $12 billion share buyback, as well as a two-for-one stock split, with shares to begin trading at the decreased price on December 24. The split will be the seventh in company history.\\nIn June 2018, Nike announced it would initiate a $15 billion share buyback over four years, to begin in 2019 upon completion of the previous buyback program.\\nFor the fiscal year 2018, Nike reported earnings of US$1.933 billion, with annual revenue of US$36.397 billion, an increase of 6.0% over the previous fiscal cycle. Nike\\'s shares traded at over $72 per share, and its market capitalization was valued at over US$114.5 billion in October 2018.\\nIn March 2020, Nike reported a 5% drop in Chinese sales associated with stores\\' closure due to the COVID-19 outbreak. It was the first decrease in six years. At the same time, the company\\'s online sales grew by 36% during Q1 of 2020. Also, the sales of personal training apps grew by 80% in China\\nProducts. Sports Apparel. Nike produces a wide range of sports equipment and apparel. Their first products were track running shoes. Nike Air Max is a line of shoes first released by Nike, Inc. in 1987. Additional product lines were introduced later, such as Air Huarache, which debuted in 1992. The most recent additions to their line are the Nike 6.0, Nike NYX, and Nike SB shoes, designed for skateboarding. Nike has recently introduced cricket shoes called Air Zoom Yorker, designed to be 30% lighter than their competitors\\'. In 2008, Nike introduced the Air Jordan XX3, a high-performance basketball shoe designed with the environment in mind.\\nNike\\'s range of products include shoes, jerseys, shorts, cleats, baselayers, etc. for sports activities such as association football, basketball, track and field, combat sports, tennis, American football, athletics, golf, ice hockey, and cross training for men, women, and children. Nike also sells shoes for activities such as skateboarding, baseball, cycling, volleyball, wrestling, cheerleading, lacrosse, cricket, aquatic activities, auto racing, and other athletic and recreational uses. Nike recently teamed up with Apple Inc. to produce the Nike+ product that monitors a runner\\'s performance via a radio device in the shoe that links to the iPod nano. While the product generates useful statistics, it has been criticized by researchers who were able to identify users\\' RFID devices from 60 feet (18 m) away using small, concealable intelligence motes in a wireless sensor network.\\nIn 2004, Nike launched the SPARQ Training Program/Division.[68] Some of Nike\\'s newest shoes contain Flywire and Lunarlite Foam to reduce weight. The Air Zoom Vomero running shoe, introduced in 2006 and currently in its 11th generation, featured a combination of groundbreaking innovations including a full-length air cushioned sole, an external heel counter, a crashpad in the heel for shock absorption, and Fit Frame technology for a stable fit.\\nIn 2023, Nike told ESPN that it would cease using kangaroo skins in its products by the end of that year and debut \"a new Nike-only, proprietary synthetic upper, [with] a new material that is a better performance solution and replaces the use of kangaroo leather.\"\\nNike Vporly. The Nike Vaporfly first came out in 2017 and their popularity, along with its performance, prompted a new series of running shoes. The Vaporfly series has a new technological composition that has revolutionized long-distance running since studies have shown that these shoes can improve marathon race time up to 4.2%. The composition of the sole contains a foamy material, Pebax, that Nike has altered and now calls it ZoomX (which can be found in other Nike products as well). Pebax foam can also be found in airplane insulation and is \"squishier, bouncier, and lighter\" than foams in typical running shoes. In the middle of the ZoomX foam there is a full-length carbon fiber plate \"designed to generate extra spring in every step\". At the time of this writing Nike had just released its newest product from the Vaporfly line, the Nike ZoomX Vaporfly NEXT%, which was marketed as \"the fastest shoe we’ve ever made\" using Nike\\'s \"two most innovative technologies, Nike ZoomX foam and VaporWeave material\".\\nStreet Fashions. The Nike brand, with its distinct V-shaped logo, quickly became regarded as a status symbol in modern urban fashion and hip-hop fashion due to its association with success in sport. Beginning in the 1980s, various items of Nike clothing became staples of mainstream American youth fashion, especially tracksuits, shell suits, baseball caps, Air Jordans, Air Force 1\\'s, and Air Max running shoes with thick, air cushioned rubber soles and contrasting blue, yellow, green, white, or red trim. Limited edition sneakers and prototypes with a regional early release were known as Quickstrikes, and became highly desirable items for teenage members of the sneakerhead subculture.\\nBy the 1990s and 2000s, American and European teenagers associated with the preppy or popular clique began combining these sneakers, leggings, sweatpants, crop tops, and tracksuits with regular casual chic street clothes such as jeans, skirts, leg warmers, slouch socks, and bomber jackets. Particularly popular were the unisex spandex Nike Tempo compression shorts worn for cycling and running, which had a mesh lining, waterproofing, and, later in the 2000s, a zip pocket for a Walkman or MP3 player. \\nFrom the late 2000s into the 2010s, Nike Elite basketball socks began to be worn as everyday clothes by hip-hop fans and young children. Originally plain white or black, these socks had special shock absorbing cushioning in the sole[96] plus a moisture wicking upper weave. Later, Nike Elite socks became available in bright colors inspired by throwback basketball uniforms, often with contrasting bold abstract designs, images of celebrities, and freehand digital print to capitalise upon the emerging nostalgia for 1990s fashion.\\nIn 2015, a new self-lacing shoe was introduced. Called the Nike Mag, which are replicas of the shoes featured in Back to the Future Part II, it had a preliminary limited release, only available by auction with all proceeds going to the Michael J. Fox Foundation. This was done again in 2016.\\nNike have introduced a premium line, focused more on streetwear than sports wear called NikeLab.\\nIn March 2017, Nike announced its launch of a plus-size clothing line, which will feature new sizes 1X through 3X on more than 200 products. Another significant development at this time was the Chuck Taylor All-Star Modern, an update of the classic basketball sneaker that incorporated the circular knit upper and cushioned foam sole of Nike\\'s Air Jordans.\\nCollectibles\\nOn July 23, 2019, a pair of Nike Inc. running shoes sold for $437,500 at a Sotheby\\'s auction. The so-called \"Moon Shoes\" were designed by Nike co-founder and track coach Bill Bowerman for runners participating in the 1972 Olympics trials. The buyer was Miles Nadal, a Canadian investor and car collector, who had just paid $850,000 for a group of 99 rare of limited collection pairs of sport shoes. The purchase price was the highest for one pair of sneakers, the previous record being $190,373 in 2017 for a pair of signed Converse shoes in California, said to have been worn by Michael Jordan during the 1984 basketball final of the Olympics that year.\\nVirtual\\nAfter acquiring RTFKT, Nike launched the Dunk Genesis Cryptokicks collection, which features over 20,000 NFTs. One design by Takashi Murakami was sold for $134,000 in April 2022\\nHeadquarters. Nike\\'s world headquarters are surrounded by the city of Beaverton but are within unincorporated Washington County. The city attempted to forcibly annex Nike\\'s headquarters, which led to a lawsuit by Nike, and lobbying by the company that ultimately ended in Oregon Senate Bill 887 of 2005. Under that bill\\'s terms, Beaverton is specifically barred from forcibly annexing the land that Nike and Columbia Sportswear occupy in Washington County for 35 years, while Electro Scientific Industries and Tektronix receive the same protection for 30 years. Nike is planning to build a 3.2 million square foot expansion to its World Headquarters in Beaverton. The design will target LEED Platinum certification and will be highlighted by natural daylight, and a gray water treatment center.\\nControversies. Nike has contracted with more than 700 shops around the world and has offices located in 45 countries outside the United States. Most of the factories are located in Asia, including Indonesia, China, Taiwan, India, Thailand, Vietnam, Pakistan, Philippines, and Malaysia. Nike is hesitant to disclose information about the contract companies it works with. However, due to harsh criticism from some organizations like CorpWatch, Nike has disclosed information about its contract factories in its Corporate Governance Report.\\nNike sweatshops\\nIn the 1990s, Nike received criticism for its use of sweatshops. Beginning in 1990, many protests occurred in big cities such as Los Angeles, Washington, DC and Boston in order to show public outcry for Nike\\'s use of child labor and sweatshops. Nike has been criticized for contracting with factories (known as Nike sweatshops) in countries such as China, Vietnam, Indonesia and Mexico. Vietnam Labor Watch, an activist group, has documented that factories contracted by Nike have violated minimum wage and overtime laws in Vietnam as late as 1996, although Nike claims that this practice has been stopped. The company has been subject to much critical coverage of the often poor working conditions and exploitation of cheap overseas labor employed in the free trade zones where their goods are typically manufactured. Sources for this criticism include Naomi Klein\\'s book No Logo and Michael Moore documentaries.\\nCampaigns have been taken up by many colleges and universities, especially anti-globalisation groups, as well as several anti-sweatshop groups such as the United Students Against Sweatshops.\\nAs of July 2011, Nike stated that two-thirds of its factories producing Converse products still do not meet the company\\'s standards for worker treatment. A July 2011 Associated Press article stated that employees at the company\\'s plants in Indonesia reported constant abuse from supervisors\\nChild labor allegations\\nDuring the 1990s, Nike faced criticism for the use of child labor in Cambodia and Pakistan in factories it contracted to manufacture soccer balls. Although Nike took action to curb or at least reduce the practice, they continue to contract their production to companies that operate in areas where inadequate regulation and monitoring make it hard to ensure that child labor is not being used.\\nIn 2001, a BBC documentary uncovered occurrences of child labor and poor working conditions in a Cambodian factory used by Nike. The documentary focused on six girls, who all worked seven days a week, often 16 hours a day.\\nStrike in China factory\\nIn April 2014, one of the biggest strikes in mainland China took place at the Yue Yuen Industrial Holdings Dongguan shoe factory, producing amongst others for Nike. Yue Yuen did underpay an employee by 250 yuan (40.82 US Dollars) per month. The average salary at Yue Yuen is 3000 yuan per month. The factory employs 70,000 people. This practice was in place for nearly 20 years\\nParadise Papers.\\nOn November 5, 2017, the Paradise Papers, a set of confidential electronic documents relating to offshore investment, revealed that Nike is among the corporations that used offshore companies to avoid taxes.\\nAppleby documents detail how Nike boosted its after-tax profits by, among other maneuvers, transferring ownership of its Swoosh trademark to a Bermudan subsidiary, Nike International Ltd. This transfer allowed the subsidiary to charge royalties to its European headquarters in Hilversum, Netherlands, effectively converting taxable company profits to an account payable in tax-free Bermuda. Although the subsidiary was effectively run by executives at Nike\\'s main offices in Beaverton, Oregon—to the point where a duplicate of the Bermudan company\\'s seal was needed—for tax purposes the subsidiary was treated as Bermuda. Its profits were not declared in Europe and came to light only because of a mostly unrelated case in US Tax Court, where papers filed by Nike briefly mention royalties in 2010, 2011 and 2012 totaling $3.86 billion. Under an arrangement with Dutch authorities, the tax break was to expire in 2014, so another reorganization transferred the intellectual property from the Bermudan company to a Dutch commanditaire vennootschap or limited partnership, Nike Innovate CV. Dutch law treats income earned by a CV as if it had been earned by the principals, who owe no tax in the Netherlands if they do not reside there\\nColin Kaepernick\\nIn September 2018, Nike announced it had signed former American football quarterback Colin Kaepernick, noted for his controversial decision to kneel during the playing of the US national anthem, to a long-term advertising campaign. According to Charles Robinson of Yahoo! Sports, Kaepernick and Nike agreed to a new contract despite the fact Kaepernick has been with the company since 2011 and said that \"interest from other shoe companies\" played a part in the new agreement. Robinson said the contract is a \"wide endorsement\" where Kaepernick will have his own branded line including shoes, shirts, jerseys and more. According to Robinson, Kaepernick signed a \"star\" contract that puts him level with a \"top-end NFL player\" worth millions per year plus royalties. In response, some people set fire to their own Nike-branded clothes and shoes or cut the Nike swoosh logo out of their clothes, and the Fraternal Order of Police called the advertisement an \"insult\"; others, such as LeBron James, Serena Williams, and the National Black Police Association, praised Nike for its campaign. The College of the Ozarks removed Nike from all their athletic uniforms in response.\\nDuring the following week, Nike\\'s stock price fell 2.2%, even as online orders of Nike products rose 27% compared with the previous year. In the following three months, Nike reported a rise in sales.\\nIn July 2019, Nike released a shoe featuring a Betsy Ross flag called the Air Max 1 Quick Strike Fourth of July trainers. The trainers were designed to celebrate Independence Day. The model was subsequently withdrawn after Colin Kaepernick told the brand he and others found the flag offensive because of its association with slavery.\\nNike\\'s decision to withdraw the product drew criticism from Arizona\\'s Republican Governor, Doug Ducey, and Texas\\'s Republican Senator Ted Cruz. Nike\\'s decision was praised by others due to the use of the flag by white nationalists, but the Anti-Defamation League\\'s Center on Extremism has declined to add the flag to its database of \"hate symbols.\"\\nHong Kong protests\\nU.S. Vice President Mike Pence criticized Nike for \"siding with the Chinese Communist Party and silencing free speech\". He claimed that after Houston Rockets general manager Daryl Morey was criticized by the Chinese government for his tweet supporting the 2019 Hong Kong protests, Nike removed Rockets merchandise from its stores in China. He stated that the brand \"promotes itself as a so called social-justice champion, but when it comes to Hong Kong, it prefers checking its social conscience at the door.\"\\nNike Vaporfly Shoe\\nMain article: Nike Vaporfly and Tokyo 2020 Olympics Controversy\\nOn January 31, 2020, the World Athletics issued new guidelines concerning shoes to be used in the upcoming Tokyo 2020 Olympics. These updates came in response to criticisms concerning technology in the Nike Vaporfly running shoes, which had been submitted beginning around 2017–2018. These criticisms stated that the shoes provided athletes with an unfair advantage over their opponents and some critics considered it to be a form of technology doping. According to Nike funded research, the shoes can improve efficiency by up to 4.2% and runners who have tested the shoe are saying that it causes reduced soreness in the legs; sports technologist Bryce Dyer attributes this to the ZoomX and carbon fiber plate since it absorbs the energy and \"spring[s] runners forward\". Some athletes, scientists, and fans have compared this to the 2008 LAZR swimsuit controversy.\\nSome of the major changes in the guidelines that have come about as a result of these criticisms include that the \"sole must be no thicker than 40mm\" and that \"the shoe must not contain more than one rigid embedded plate or blade (of any material) that runs either the full length or only part of the length of the shoe. The plate may be in more than one part but those parts must be located sequentially in one plane (not stacked or in parallel) and must not overlap\". The components of the shoes are not the only thing that had major changes; starting April 30, 2020, \"any shoe must have been available for purchase by any athlete on the open retail market (online or in store) for a period of four months before it can be used in competition\". Prior to these new guidelines World Athletics reviewed the Vaporfly shoes and \"concluded that there is independent research that indicates that the new technology incorporated in the soles of road and spiked shoes may provide a performance advantage\" and that it recommends further research to \"establish the true impact of [the Vaporfly] technology.\"\\nLil Nas X Satan Shoes\\nOn March 29, 2021, American rapper Lil Nas X partnered with New York–based art collective MSCHF to release a modified pair of Nike Air Max 97s called Satan Shoes. The shoes are black and red with a bronze pentagram, featuring the Bible verse Luke 10:18 and are filled with \"60cc and 1 drop of human blood.\" Only 666 pairs were created and were sold at a price of $1,018. Nike immediately iterated that they were uninvolved in the creation and promotion of the shoes and did not endorse the messages of Lil Nas X or MSCHF. Nike filed a trademark lawsuit against MSCHF with the New York federal Court, in an effort to stop the distribution of the shoes. On April 1, a federal judge ordered a temporary restraining order blocking the sale and distribution of the shoes pending a preliminary injunction.\\nForced Uyghur labor allegations\\nIn December 2021, the European Center for Constitutional and Human Rights filed a criminal complaint in a Dutch court against Nike and other brands, alleging that they benefited from the use of forced Uyghur labor in Xinjiang. In July 2023, the Canadian Ombudsperson for Responsible Enterprise opened an investigation into Nike to probe allegations of forced Uyghur labor in its supply chain.\\nEnvirontmental Record\\nIn 2007, New England–based environmental organization Clean Air-Cool Planet ranked Nike among the top three companies (out of 56) in a survey of climate-friendly companies.\\nRecycling\\nNike has also been praised for its Nike Grind program, which closes the product lifecycle, by groups such as Climate Counts.\\nSince 1993, Nike has worked on its Reuse-A-Shoe program. This program is Nike\\'s longest-running program that benefits both the environment and the community by collecting old athletic shoes of any type in order to process and recycle them. The material that is produced is then used to help create sports surfaces such as basketball courts, running tracks, and playgrounds. Nike France made their Reuse-A-Shoe program available online so that they could make it easier for consumers to send in their old shoes. In 2017, it was estimated that 28,000,000 shoes were collected since its start in 1993. Nike limited the mail-in option of the program because they are aware that the emissions from shipping would offset the good, they are trying to do. They work with the National Recycling Coalition to help limit transportation of recycled shoes. During transportation most of the vehicles that are used are using diesel or fuel oil. Diesel oil emits 22.44 pounds of Carbon Dioxide per gallon.\\nA campaign that Nike began for Earth Day 2008 was a commercial that featured basketball star Steve Nash wearing Nike\\'s Trash Talk Shoe, which had been constructed in February 2008 from pieces of leather and synthetic leather waste from factory floors. The Trash Talk Shoe also featured a sole composed of ground-up rubber from a shoe recycling program. Nike claims this is the first performance basketball shoe that has been created from manufacturing waste, but it only produced 5,000 pairs for sale.\\nSulfur hexafluoride\\nSulfur hexafluoride is an extremely potent and persistent greenhouse gas that was used to fill the cushion bags in all \"Air\"-branded shoes from 1992 to 2006. 277 tons was used during the peak in 1997.\\nToxic chemicals\\nIn 2008, a project through the University of North Carolina at Chapel Hill found workers were exposed to toxic isocyanates and other chemicals in footwear factories in Thailand. In addition to inhalation, dermal exposure was the biggest problem found. This could result in allergic reactions including asthmatic reactions.[\\nWater pollution\\nIn July 2011, environmental group Greenpeace published a report regarding water pollution impacting the Yangtze River emitted from a major textile factory operated by Nike supplier Youngor Group. Following the report, Nike, as well as Adidas, Puma, and a number of other brands included in the report announced an agreement to stop discharging hazardous chemicals by 2020. However, in July 2016 Greenpeace released a follow-up report which found that Nike \"does not take individual responsibility\" for eliminating hazardous chemicals, stating that Nike had not made an explicit commitment to riding itself of perfluorinated compounds, and that \"Nike does not ensure its suppliers report their hazardous chemical discharge data and has not made a commitment to do so\".\\nBack in 2016 Nike started to use water free dyeing materials so that they can help reduce their water use in their Southeast Asian factories.\\nCarbon footprint\\nNike reported Total CO2e emissions (Direct + Indirect) for the twelve months ending 30 June 2020 at 317 Kt and plans to reduce emissions 65% by 2030 from a 2015 base year. This science-based target is aligned with Paris Agreement to limit global warming to 1.5 °C above pre-industrial levels. According to a study done in 2017, Nike contributed 3,002,529 metric tons of Carbon Dioxide in 2017 combined from different sectors in the company like retail, manufacturing, management, and more.\\nPartnership with Newlight\\nIn 2021, Nike announced they were working with Newlight Technologies to find more eco-friendly materials for their sneakers. They specifically mentioned Newlight’s AirCarbon product which is a bioplastic that can be used to make shoes. The bioplastic is used as a replacement to leather, plastic, and other materials that are like that. Newlight was reported saying that the goal is to reduce Nike’s carbon footprint.\\nSustainability\\nNike Inc. has been committed to fighting climate change by becoming a sustainable business. Even as they continue to grow, they have still achieved reducing their environmental impact. The company acknowledges both triumphs and problems in its FY12-13 Sustainable Business Performance Summary, while they are making progress in major impact areas such as climate and energy, labor, chemistry, water, waste, and community. The company\\'s progress is evidenced by an absolute reduction in carbon emissions of close to 3% across the whole value chain from its FY11 baseline, though sales increased by 26% during the same year. Production increased as the company met its strategic goal of sourcing from fewer, higher-performing contract factories, with a 14% decrease – from 910 to 785 plants during the last two years.\\nNike Inc. is investing in sustainable materials and production techniques in addition to lowering their carbon footprint to combat climate change. A variety of environmentally friendly items created with materials including recycled polyester, organic cotton, and water-based adhesives have been introduced by Nike. For fiscal year 2015, the company enhanced water efficiency by 15% per unit in clothing materials dyeing and finishing and footwear manufacturing, as well as achieved a 20% reduction in CO2 emissions and a 10% decrease in waste. Additionally, they have used cutting-edge manufacturing techniques that lessen waste production and the overall environmental impact of their products.\\nIn 2019, Nike began an innovative program called \"Move to Zero,\" which is an effort to achieve zero waste and zero carbon in the organization\\'s supply chain and product lifetime. The project comprises a variety of tactics, including boosting the use of sustainable materials, investing in renewable energy, and enhancing energy efficiency. The efficiency of designs calculated for the Move to Zero collection is 90% or higher, which means that almost all of the fabric used in production ends up in the garment rather than in the trash. The men\\'s and women\\'s sections of the collection contain at least 60% organic and recycled materials, including sustainably sourced cotton\\nMarketing strategy\\nNike promotes its products through sponsorship agreements with celebrity athletes, professional teams and college athletic teams.\\nAdvertising\\nIn 1982, Nike aired its first three national television ads, created by newly formed ad agency Wieden+Kennedy (W+K), during the broadcast of the New York Marathon. The Cannes Advertising Festival has named Nike its Advertiser of the Year in 1994 and 2003, making it the first company to receive that honor twice.\\nNike also has earned the Emmy Award for best commercial in 2000 and 2002. The first was for \"The Morning After,\" a satirical look at what a runner might face on the morning of January 1, 2000, if every dire prediction about the Y2K problem came to fruition. The second was for a 2002 spot called \"Move,\" which featured a series of famous and everyday athletes in a variety of athletic pursuits.\\nBeatles song\\nNike was criticized for its use of the Beatles song \"Revolution\" in a 1987 commercial against the wishes of Apple Records, the Beatles\\' recording company. Nike paid US$250,000 to Capitol Records Inc., which held the North American licensing rights to the recordings, for the right to use the Beatles\\' rendition for a year.\\nThat same year, Apple Records sued Nike Inc., Capitol Records Inc., EMI Records Inc. and Wieden+Kennedy for $15 million. Capitol-EMI countered by saying the lawsuit was \"groundless\" because Capitol had licensed the use of \"Revolution\" with the \"active support and encouragement of Yoko Ono, a shareholder and director of Apple Records.\"\\nNike discontinued airing ads featuring \"Revolution\" in March 1988. Yoko Ono later gave permission to Nike to use John Lennon\\'s \"Instant Karma\" in another advertisement.\\nNew media marketing\\nNike was an early adopter of internet marketing, email management technologies, and using broadcast and narrowcast communication technologies to create multimedia marketing campaigns.\\nMinor Threat advertisement\\nIn late June 2005, Nike received criticism from Ian MacKaye, owner of Dischord Records, guitarist/vocalist for Fugazi and The Evens, and front man of the defunct punk band Minor Threat, for appropriating imagery and text from Minor Threat\\'s 1981 self-titled album\\'s cover art in a flyer promoting Nike Skateboarding\\'s 2005 East Coast demo tour.\\nOn June 27, Nike Skateboarding\\'s website issued an apology to Dischord, Minor Threat, and fans of both and announced that they have tried to remove and dispose of all flyers. They stated that the people who designed it were skateboarders and Minor Threat fans themselves who created the advertisement out of respect and appreciation for the band. The dispute was eventually settled out of court between Nike and Minor Threat.\\nNike 6.0\\nAs part of the 6.0 campaign, Nike introduced a new line of T-shirts that include phrases such as \"Dope\", \"Get High\" and \"Ride Pipe\" – sports lingo that is also a double entendre for drug use. Boston Mayor Thomas Menino expressed his objection to the shirts after seeing them in a window display at the city\\'s Niketown and asked the store to remove the display. \"What we don\\'t need is a major corporation like Nike, which tries to appeal to the younger generation, out there giving credence to the drug issue,\" Menino told The Boston Herald. A company official stated the shirts were meant to pay homage to extreme sports, and that Nike does not condone the illegal use of drugs. Nike was forced to replace the shirt line.\\nNBA uniform deal\\nIn June 2015, Nike signed an 8-year deal with the NBA to become the official uniform supplier for the league, beginning with the 2017–18 season. The brand took over for Adidas, who provided the uniforms for the league since 2006. Unlike previous deals, Nike\\'s logo appear on NBA jerseys – a first for the league. Initially, the Charlotte Hornets, owned by longtime Nike endorser Michael Jordan, were the only team not to sport the Nike swoosh, instead wearing the Jumpman logo associated with Jordan-related merchandise. However, beginning with the 2020–21 season, the Jumpman replaced the swoosh on the NBA\\'s alternate \"Statement\" uniforms.\\nSponsorship\\nNike sponsors top athletes in many sports to use their products and promote and advertise their technology and design. Nike\\'s first professional athlete endorser was Romanian tennis player Ilie Năstase. The first track endorser was distance runner Steve Prefontaine. Prefontaine was the prized pupil of the company\\'s co-founder, Bill Bowerman, while he coached at the University of Oregon. Today, the Steve Prefontaine Building is named in his honor at Nike\\'s corporate headquarters. Nike has only made one statue of its sponsored athletes and it is of Steve Prefontaine.\\nNike has also sponsored many other successful track and field athletes over the years, such as Sebastian Coe, Carl Lewis, Jackie Joyner-Kersee, Michael Johnson and Allyson Felix. The signing of basketball player Michael Jordan in 1984, with his subsequent promotion of Nike over the course of his career, with Spike Lee as Mars Blackmon, proved to be one of the biggest boosts to Nike\\'s publicity and sales\\nNike is a major sponsor of the athletic programs at Penn State University and named its first child care facility after Joe Paterno when it opened in 1990 at the company\\'s headquarters. Nike originally announced it would not remove Paterno\\'s name from the building in the wake of the Penn State sex abuse scandal. After the Freeh Report was released on July 12, 2012, Nike CEO Mark Parker announced the name Joe Paterno would be removed immediately from the child development center. A new name has yet to be announced.\\nIn the early 1990s, Nike made a strong push into the association football business making endorsement deals with famous and charismatic players such as Romário, Eric Cantona or Edgar Davids. They continued the growth in the sport by signing more top players including: Ronaldo, Ronaldinho, Francesco Totti, Thierry Henry, Didier Drogba, Andrés Iniesta, Wayne Rooney and still have many of the sport\\'s biggest stars under their name, with Cristiano Ronaldo, Zlatan Ibrahimović, Neymar, Harry Kane, Eden Hazard and Kylian Mbappé among others. A Barcelona prodigy, Lionel Messi had been signed with Nike since age 14, but transferred to Adidas after they successfully challenged their rival\\'s claim to his image rights in court\\nNike has been the official ball supplier for the Premier League since the 2000–01 season. In 2012, Nike carried a commercial partnership with the Asian Football Confederation. In August 2014, Nike announced that they will not renew their kit supply deal with Manchester United after the 2014–15 season, citing rising costs. Since the start of the 2015–16 season, Adidas has manufactured Manchester United\\'s kit as part of a world-record 10-year deal worth a minimum of £750 million.\\nNike still has many of the top teams playing in their uniforms, including: FC Barcelona, Paris Saint-Germain and Liverpool (the latter from the 2020–21 season), and the national teams of Brazil, France, England, Portugal and the Netherlands among many others.\\nNike has been the sponsor for many top ranked tennis players. Brand\\'s commercial success in the sport went hand in hand with the endorsement deals signed with the biggest and the world\\'s most charismatic stars and number one ranked players of the subsequent eras, including John McEnroe in the 1980s, Andre Agassi and Pete Sampras in the 1990s and Roger Federer, Rafael Nadal, Serena Williams and Maria Sharapova with the start of the 21st century.\\nNike has sponsored Tiger Woods for much of his career, and remained on his side amid the controversies that shaped the golfer\\'s career. In January 2013, Nike signed Rory McIlroy, the then No 1 golfer in the world to a 10-year sponsorship deal worth $250 million. Nike has also gone on to sign top players in golf including Scottie Scheffler, Nelly Korda, Tommy Fleetwood, Tony Finau, Jason Day and Francesco Molinari.\\nNike was the official kit sponsor for the Indian cricket team from 2005 to 2020. On February 21, 2013, Nike announced it suspended its contract with South African limbless athlete Oscar Pistorius, due to him being charged with premeditated murder.\\nNike consolidated its position in basketball in 2015 when it was announced that the company would sign an 8-year deal with the NBA, taking over from the league\\'s previous uniform sponsor, Adidas. The deal required all franchise team members to wear jerseys and shorts with the Swoosh logo, beginning with the 2017/18 season. After the success of partnership with Jordan, which resulted in the creation of the unique Air Jordan brand, Nike has continued to build partnership with the biggest names in basketball. LeBron James was given the Slogan \"We are All Witnesses\" when he signed with Nike. Similar to \"Air Jordan\", LeBron\\'s brand became massively popular. The slogan was an extremely accurate way to describe the situation LeBron was heading into in the NBA as he was expected to be the new king of the NBA. Some have had signature shoes designed for them, including Kobe Bryant, Jason Kidd, Vince Carter and more recently LeBron James, Kevin Durant, Giannis Antetokounmpo and Paul George, among others.\\nA news report originating from CNN reported that Nike spent $11.5 billion, nearly a third of its sales, on marketing and endorsement contracts in the year 2018. Nike and its Jordan brand sponsored 85 men\\'s and women\\'s basketball teams in the NCAA tournament.\\nTies with the University of Oregon\\nNike maintains strong ties, both directly and through partnerships with Phil Knight, with the University of Oregon. Nike designs the University of Oregon football program\\'s team attire. New unique combinations are issued before every game day. Tinker Hatfield, who also redesigned the university\\'s logo, leads this effort.\\nMore recently, the corporation donated $13.5 million towards the renovation and expansion of Hayward Field.\\nPhil Knight has invested substantial personal funds towards developing and maintaining the university\\'s athletic apparatus. His university projects often involve input from Nike designers and executives, such as Tinker Hatfield.\\nCauses\\nIn 2012, Nike is listed as a partner of the (PRODUCT)RED campaign together with other brands such as Girl, American Express, and Converse. The campaign\\'s mission is to prevent the transmission of HIV from mother to child. The campaign\\'s byline is \"Fighting For An AIDS Free Generation\". The company\\'s goal is to raise and send funds, for education and medical assistance to those who live in areas heavily affected by AIDS. In 2023, Nike became the presenting sponsor of Reviving Baseball in Inner Cities, which encourages youth in underserved communities to participate in baseball and softball.\\nProgram\\nThe Nike Community Ambassador Program, allows Nike employees from around the world to go out and give to their community. Over 3,900 employees from various Nike stores have participated in teaching children to be active and healthy'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/fullText/nike_data_fulltext.csv')\n",
    "data = df[\"Text\"].str.cat(sep='\\n')\n",
    "zz = re.sub(r'\\n+', '\\n', data).strip()  # Remove excess newline characters\n",
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2780"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unwanted characters\n",
    "cleaned_data = zz.replace('\\n', ' ').replace('\\'', '')\n",
    "#cleaned_data\n",
    "# Split the text into words\n",
    "word_array = cleaned_data.split()\n",
    "print(len(word_array))\n",
    "# Convert the word_array into a set to get unique values\n",
    "unique_words = set(word_array)\n",
    "\n",
    "# Calculate the number of unique words\n",
    "unique_word_count = len(unique_words)\n",
    "unique_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNvBu4Jj2F8tAESALuZqmkV",
   "include_colab_link": true,
   "mount_file_id": "1WRB2uz0Mpfkw6AnDAxCOPwY_TT2tRizG",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
